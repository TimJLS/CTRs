{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))\n",
    "# sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "import hyperopt as hpt\n",
    "import modelconfig\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def cv(self):\n",
    "        pass\n",
    "\n",
    "    def re_fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self):\n",
    "        pass\n",
    "\n",
    "    def load(self, model_file=None):\n",
    "        if model_file is None:\n",
    "            if hasattr(self, 'model_file'):\n",
    "                model_file = self.model_file\n",
    "            else:\n",
    "                raise ValueError\n",
    "        self.model = joblib.load(model_file)\n",
    "\n",
    "    def dump(self, name, model_file=None):\n",
    "        if not str(name).endswith(\".model\"):\n",
    "            name = str(name) + \".model\"\n",
    "        if model_file is None:\n",
    "            if hasattr(self, 'model_file'):\n",
    "                model_file = self.model_file\n",
    "            else:\n",
    "                raise ValueError\n",
    "        model_file = model_file + '/'+ str(name)\n",
    "        joblib.dump(self.model, model_file)\n",
    "\n",
    "    def history(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tunner(dict):\n",
    "    def __init__(self, train_set, objective, params, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        self.params = params\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<{0}> {1}\".format(\n",
    "            self.__class__.__name__,\n",
    "            json.dumps(\n",
    "                self.export_value(self.__dict__),\n",
    "                sort_keys=True,\n",
    "                indent=4,\n",
    "                separators=(',', ': '),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def export_value(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            data = dict((k, self.export_value(v))\n",
    "                        for k, v in data.items()\n",
    "                        if not k.startswith('_'))\n",
    "        elif isinstance(data, list):\n",
    "            data = [self.export_value(v) for v in data]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBMTunner(dict):\n",
    "    def __init__(self, objective, train_set, params, label=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        if isinstance(train_set, dataset.Dataset):\n",
    "            if label is not None:\n",
    "                self.train_set = lgb.Dataset(train_set._data, label)\n",
    "            elif train_set._label is not None:\n",
    "                self.train_set = lgb.Dataset(train_set._data, train_set._label)\n",
    "            else:\n",
    "                self.train_set = lgb.Dataset(train_set._data)\n",
    "\n",
    "        elif isinstance(train_set, lgb.Dataset):\n",
    "            self.train_set = train_set\n",
    "        self.params = params\n",
    "        self.objective = objective\n",
    "        self.trials = hpt.Trials()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<{0}> {1}\".format(\n",
    "            self.__class__.__name__,\n",
    "            json.dumps(\n",
    "                self.export_value(self.params),\n",
    "                sort_keys=True,\n",
    "                indent=4,\n",
    "                separators=(',', ': '),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def export_value(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            data = dict((k, self.export_value(v))\n",
    "                        for k, v in data.items()\n",
    "                        if not k.startswith('_'))\n",
    "        elif isinstance(data, list):\n",
    "            data = [self.export_value(v) for v in data]\n",
    "        return data\n",
    "\n",
    "    def full_objective(self, params):\n",
    "        if 'nfold' not in params:\n",
    "            result = self.objective(params, self.train_set, nfold=10)\n",
    "        else:\n",
    "            result = self.objective(params, self.train_set)\n",
    "        best_score = np.max(result['auc-mean'])\n",
    "        loss = 1 - best_score\n",
    "        n_estimators = int(np.argmax(result['auc-mean']) + 1)\n",
    "        return {'loss': loss, 'params': params,\n",
    "                'estimators': n_estimators, \n",
    "                'status': hpt.STATUS_OK}\n",
    "\n",
    "    def start(self, max_evals=100):\n",
    "        best = hpt.fmin(fn=self.full_objective,\n",
    "                        space=self.params,\n",
    "                        algo=hpt.tpe.suggest,                      \n",
    "                        max_evals=max_evals, \n",
    "                        trials=self.trials,\n",
    "                        rstate=np.random.RandomState(27))\n",
    "        return best\n",
    "\n",
    "    def show_best(self):\n",
    "        return pd.DataFrame(self.trials.best_trial).result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGBModel(Model):\n",
    "    def __init__(self, params=None, model_file=None, silent=False):\n",
    "        \"\"\"Initialize the LightGBM Booster.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dict or None, optional (default=None)\n",
    "            Parameters for Booster.\n",
    "        train_set : Dataset or None, optional (default=None)\n",
    "            Training dataset.\n",
    "        model_file : string or None, optional (default=None)\n",
    "            Path to the model file.\n",
    "        model_str : string or None, optional (default=None)\n",
    "            Model will be loaded from this string.\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.trials = hpt.Trials()\n",
    "        self.model_file = model_file\n",
    "\n",
    "    def train(self, train_set, params=None, num_boost_round=100,\n",
    "              valid_sets=None, valid_names=None, *arg, **kwargs):\n",
    "        if params is None and self.params is None:\n",
    "            raise ValueError\n",
    "        elif params:\n",
    "            self.params = copy.deepcopy(params)\n",
    "\n",
    "        if valid_sets is not None:\n",
    "            if isinstance(valid_sets, dataset.Dataset):\n",
    "                valid_sets = [lgb.Dataset(valid_set._data, valid_set._label)]\n",
    "            elif isinstance(valid_sets, lgb.Dataset):\n",
    "                valid_sets = [valid_sets]\n",
    "            elif not isinstance(valid_sets, list):\n",
    "                raise TypeError\n",
    "            for i, valid_set in enumerate(valid_sets):\n",
    "                if isinstance(valid_set, dataset.Dataset):\n",
    "                    valid_sets[i] = lgb.Dataset(valid_set._data, valid_set._label)\n",
    "                if isinstance(valid_set, lgb.Dataset):\n",
    "                    valid_sets[i] = valid_set\n",
    "\n",
    "        if 'nfold' in self.params:\n",
    "            self.params.pop('nfold', None)\n",
    "        if 'num_boost_round' in self.params:\n",
    "            self.params.pop('num_boost_round', None)\n",
    "\n",
    "        if isinstance(train_set, dataset.Dataset):\n",
    "            train_set.construct()\n",
    "            self.train_set = lgb.Dataset(train_set._data, train_set._label)\n",
    "        elif isinstance(train_set, lgb.Dataset):\n",
    "            self.train_set = train_set\n",
    "        else:\n",
    "            raise TypeError\n",
    "        self.model = lgb.train(params=self.params,\n",
    "                               train_set=self.train_set,\n",
    "                               valid_sets=valid_sets,\n",
    "                               valid_names=valid_names,\n",
    "                               *arg, **kwargs)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, data):\n",
    "        if isinstance(data, lgb.Dataset):\n",
    "            raise TypeError\n",
    "        elif isinstance(data, dataset.Dataset):\n",
    "            data = data._data\n",
    "        predicts = self.model.predict(data)\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test():\n",
    "    ## construct dataset\n",
    "    df = dataset.get_train_lookup()\n",
    "    label = df.pop('is_over_kpi')\n",
    "    categorical_feature = ['industry', 'type', 'is_avg_over_kpi']\n",
    "    params = {\n",
    "        'numerical_cols': ['kpi_value', 'campaign_period', 'avg_spend_cap', 'avg_cpm', 'avg_ctr',\n",
    "                           'day_spend', 'audience_size'],\n",
    "        'scale_cols': ['kpi_value', 'campaign_period', 'avg_spend_cap', 'avg_cpm', 'avg_ctr',\n",
    "                       'day_spend', 'audience_size'],\n",
    "        'label_encode_cols': ['interest_id']\n",
    "    }\n",
    "    trainset = dataset.Dataset(df, label=label, categorical_feature=categorical_feature, params=params)\n",
    "    trainset.construct()\n",
    "    # split into train, eval, test\n",
    "    trainset, validset, testset = trainset.train_eval_test_split(eval_size=0.3,\n",
    "                                                                 test_size=0.3,\n",
    "                                                                 random_state=27)\n",
    "    trainset.construct(), validset.construct(), testset.construct()\n",
    "\n",
    "    # parameter tuning\n",
    "    space = {\n",
    "        'boosting_type': hpt.hp.choice('boosting_type', ['gbdt', 'goss']),\n",
    "        'num_leaves': hpt.hp.choice('num_leaves', np.arange(2, 15+1, dtype=int)),\n",
    "        'max_depth': hpt.hp.choice('max_depth', np.arange(2, 10+1, dtype=int)),\n",
    "        'learning_rate': hpt.hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "        'seed': 27,\n",
    "        'nfold': 10,\n",
    "        'metrics': 'auc',\n",
    "        'num_boost_round': 1000,\n",
    "        'early_stopping_rounds': 100,\n",
    "    }\n",
    "\n",
    "    tuner = LGBMTunner(objective=lgb.cv,\n",
    "                       train_set=trainset,\n",
    "                       params=space)\n",
    "    tuner.start(max_evals=10)\n",
    "\n",
    "    ## use best params of tunner to train a lgb booster\n",
    "    best_hp = tuner.show_best()\n",
    "    best_hp['objective'] = 'binary'\n",
    "    best_m = LGBModel(params=best_hp)\n",
    "\n",
    "    best_m.train(trainset,\n",
    "                 params=best_hp,\n",
    "                 valid_sets=[trainset, validset],\n",
    "                 valid_names=['train', 'eval'],\n",
    "                 num_boost_round=100)\n",
    "    best_m.dump('test', model_file='ind_lgb_3/')\n",
    "    # predict using testset\n",
    "    predicts = best_m.predict(testset)\n",
    "    predictions = pd.DataFrame({'class': predicts})\n",
    "    predictions['class'] = predictions['class'].apply(lambda x : 1 if x >= 0.5 else 0)\n",
    "#     f1_score(test_label.values, predictions)\n",
    "    return predictions, best_m, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    predictions, best_m, testset = pipeline_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook model.ipynb to script\n",
      "[NbConvertApp] Writing 9705 bytes to model.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
